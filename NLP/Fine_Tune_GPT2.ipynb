{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tune-GPT2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCJJLiSTdTW2"
      },
      "source": [
        "# Introduction to GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvh656HcdWVW"
      },
      "source": [
        "[Open this notebook on Colab in Playground Mode](https://colab.research.google.com/drive/1Qzch1S3_7OaqxOwgoP52w0MkumuONe6T#scrollTo=PCJJLiSTdTW2&forceEdit=true&sandboxMode=true)\n",
        "\n",
        "_**What is GPT2?**_\n",
        "\n",
        "GPT2 (Generative Pre-trained Transformer 2) is a language model that was created by OpenAI in February 2019.\n",
        "\n",
        "The original GPT2 is a (extra) large transformer-based language model with 1.5 billion parameters that was trained on a dataset of 8 million web pages. The GPT2 model also comes in different sizes that include small (117M), medium (345M) and large (762M) parameters.\n",
        "\n",
        "![GPT2 Image Sizes](https://jalammar.github.io/images/gpt2/gpt2-sizes.png)\n",
        "\n",
        "<br />\n",
        "\n",
        "_**What was it trained on?**_\n",
        "\n",
        "GPT2 was trained to predict the next word in 40GB of \"Internet text\". This \"Internet text\" was obtained from Reddit. The dataset was taken from web links, and the data is text. Hence, the dataset was called WebText.\n",
        "\n",
        "<br />\n",
        "\n",
        "_**The difference between GPT2 and BERT?**_\n",
        "\n",
        "The GPT2 consists of transformer decoder blocks while BERT uses transformer encoder blocks. This makes BERT very good at fill-in-the-blanks while GPT2 is very good at writing essays.\n",
        "\n",
        "<br />\n",
        "\n",
        "_**References**_\n",
        "\n",
        "* Learn about GPT2\n",
        "    * [The Illustrated GPT-2 (Visualizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/)\n",
        "    * [How to Build OpenAI's GPT-2: \"The AI That Was Too Dangerous to Release\"](https://blog.floydhub.com/gpt2/)\n",
        "    * [Better Language\n",
        "Models and Their\n",
        "Implications](https://openai.com/blog/better-language-models/)\n",
        "\n",
        "* Code\n",
        "    * [Official PyTorch Example](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py)\n",
        "    * [Fine-tuning GPT2 for Text Generation Using Pytorch](https://towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7)\n",
        "    * [Fine-Tuning GPT2 on Colab GPUâ€¦ For Free!](https://towardsdatascience.com/fine-tuning-gpt2-on-colab-gpu-for-free-340468c92ed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "118SrWPqZI_b"
      },
      "source": [
        "# Download Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAQmxQdPZLxD",
        "outputId": "62adfca9-bd2b-49bb-e716-f0da53dd5f96"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.2 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.7)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.2->datasets) (3.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOPuSXwvQcBP"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcsM7dqSQdhP"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import transformers\n",
        "import math\n",
        "\n",
        "from datasets import load_dataset\n",
        "from dataclasses import dataclass\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeB60oUWUWof"
      },
      "source": [
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ArHY60uh3Jw"
      },
      "source": [
        "Configuration is as follows:\n",
        "\n",
        "* Model Configuration -\n",
        "    * model_name: Name of the language model.\n",
        "    * dataset_name: The name of the dataset to use (via the datasets library).\n",
        "    * dataset_config_name: The configuration name of the dataset to use (via the datasets library).\n",
        "    * output_dir: The output directory where the model predictions and checkpoints will be written.\n",
        "    * overwrite_output_dir: Overwrite the content of the output directory.\n",
        "\n",
        "* Training Configuration -\n",
        "    * per_device_train_batch_size: The batch size per GPU/TPU core/CPU for training.\n",
        "    * save_steps: Number of updates steps before two checkpoint saves.\n",
        "    * num_train_epochs: Total number of training epochs to perform.\n",
        "\n",
        "* Preprocessing Configuration -\n",
        "    * fast_tokenizer: Whether or not to try to load the fast version of the tokenizer.\n",
        "    * num_workers: The number of processes to use for the preprocessing.\n",
        "    * overwrite_cache: Overwrite the cached training and evaluation sets.\n",
        "    * max_train_samples: For debugging purposes or quicker training, truncate the number of training examples to this.\n",
        "    * max_val_samples: For debugging purposes or quicker training, truncate the number of validation examples to this.\n",
        "    * block_size: Optional input sequence length after tokenization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_JEkAS-h27u"
      },
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "\n",
        "    model_name: str = 'gpt2'\n",
        "    dataset_name: str = 'wikitext'\n",
        "    dataset_config_name: str = 'wikitext-2-raw-v1'\n",
        "    output_dir: str = '/tmp/test-clm'\n",
        "    overwrite_output_dir: bool = False\n",
        "    \n",
        "    per_device_train_batch_size: int = 1\n",
        "    save_steps: int = -1\n",
        "    num_train_epochs: int = 1\n",
        "\n",
        "    fast_tokenizer: bool = True\n",
        "    num_workers: int = None\n",
        "    overwrite_cache: bool = False\n",
        "    max_train_samples: int = None\n",
        "    max_val_samples: int = None\n",
        "    block_size: int = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WImE1TeszoJE"
      },
      "source": [
        "config = Config(\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_totjcOY66v"
      },
      "source": [
        "# Fine-Tune GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1X3NfNZZYNM"
      },
      "source": [
        "Prepare dataset defined by HuggingFace\n",
        "\n",
        "Snippets taken from the official HuggingFace example: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTPN1-t7VNTO"
      },
      "source": [
        "def get_dataset(config):\n",
        "    dataset = load_dataset(config.dataset_name, config.dataset_config_name)\n",
        "\n",
        "    conf = transformers.AutoConfig.from_pretrained(config.model_name)\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "        config.model_name, use_fast=config.fast_tokenizer\n",
        "    )\n",
        "\n",
        "    # Tokenize text\n",
        "    column_names = dataset[\"train\"].column_names\n",
        "    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
        "\n",
        "    def tokenize_function(example):\n",
        "        return tokenizer(example[text_column_name])\n",
        "\n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        num_proc=config.num_workers,\n",
        "        remove_columns=column_names,\n",
        "        load_from_cache_file=not config.overwrite_cache\n",
        "    )\n",
        "\n",
        "    if config.block_size is None:\n",
        "        block_size = tokenizer.model_max_length\n",
        "        if block_size > 1024:\n",
        "            logger.warn(\n",
        "                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
        "                \"Picking 1024 instead. You can change that default value by setting block_size in config.\"\n",
        "            )\n",
        "        block_size = 1024\n",
        "    else:\n",
        "        if config.block_size > tokenizer.model_max_length:\n",
        "            logger.warn(\n",
        "                f\"The block_size passed ({config.block_size}) is larger than the maximum length for the model\"\n",
        "                f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
        "            )\n",
        "        block_size = min(config.block_size, tokenizer.model_max_length)\n",
        "\n",
        "    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
        "    def group_texts(examples):\n",
        "        # Concatenate all texts.\n",
        "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "        total_length = (total_length // block_size) * block_size\n",
        "        # Split by chunks of max_len.\n",
        "        result = {\n",
        "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "            for k, t in concatenated_examples.items()\n",
        "        }\n",
        "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "        return result\n",
        "\n",
        "    lm_dataset = tokenized_dataset.map(\n",
        "        group_texts,\n",
        "        batched=True,\n",
        "        num_proc=config.num_workers,\n",
        "        load_from_cache_file=not config.overwrite_cache,\n",
        "    )\n",
        "\n",
        "    return tokenized_dataset, lm_dataset, tokenizer, conf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RfNyx3xiwam"
      },
      "source": [
        "Train method -\n",
        "\n",
        "This will get the dataset configured in the Config dataclass and split it into train and eval dataset. After this, the Trainer will run the training and evaluation loops."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pkHhbkPv8mG"
      },
      "source": [
        "def train(config):\n",
        "    print(\"Getting Dataset...\")\n",
        "    tokenized_dataset, lm_dataset, tokenizer, conf = get_dataset(config)\n",
        "\n",
        "    if \"train\" not in tokenized_dataset:\n",
        "        raise ValueError(\"Training requires a train dataset\")\n",
        "    train_dataset = lm_dataset[\"train\"]\n",
        "    if config.max_train_samples is not None:\n",
        "        train_dataset = train_dataset.select(range(config.max_train_samples))\n",
        "\n",
        "    if \"validation\" not in tokenized_dataset:\n",
        "        raise ValueError(\"Validation requires a validation dataset\")\n",
        "    eval_dataset = lm_dataset[\"validation\"]\n",
        "    if config.max_val_samples is not None:\n",
        "        eval_dataset = eval_dataset.select(range(config.max_val_samples))\n",
        "    print(\"Dataset Prepared!\")\n",
        "\n",
        "    print(\"Loading Model...\")\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "            config.model_name, config=conf\n",
        "    )\n",
        "    print(\"Model Loaded!\")\n",
        "\n",
        "    \n",
        "    # Initialize our Trainer\n",
        "    print(\"Starting Training...\")\n",
        "    training_args = transformers.TrainingArguments(\n",
        "        output_dir=config.output_dir,\n",
        "        overwrite_output_dir=config.overwrite_output_dir,\n",
        "        per_device_train_batch_size=config.per_device_train_batch_size,\n",
        "        num_train_epochs=config.num_train_epochs,\n",
        "        save_steps=config.save_steps\n",
        "    )\n",
        "    \n",
        "    trainer = transformers.Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset if train else None,\n",
        "        eval_dataset=eval_dataset if eval else None,\n",
        "        tokenizer=tokenizer,\n",
        "        # Data collator will default to DataCollatorWithPadding, so we change it.\n",
        "        data_collator=transformers.default_data_collator,\n",
        "    )\n",
        "\n",
        "    train_result = trainer.train()\n",
        "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "    metrics = train_result.metrics\n",
        "\n",
        "    max_train_samples = (\n",
        "        config.max_train_samples if config.max_train_samples is not None else len(train_dataset)\n",
        "    )\n",
        "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"Starting Evaluation...\")\n",
        "\n",
        "    metrics = trainer.evaluate()\n",
        "\n",
        "    max_val_samples = config.max_val_samples if config.max_val_samples is not None else len(eval_dataset)\n",
        "    metrics[\"eval_samples\"] = min(max_val_samples, len(eval_dataset))\n",
        "    perplexity = math.exp(metrics[\"eval_loss\"])\n",
        "    metrics[\"perplexity\"] = perplexity\n",
        "\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    print(f\"Eval Loss: {metrics['eval_loss']}\\nPerplexity: {metrics['perplexity']}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "sRp3zEqm-i6P",
        "outputId": "ec1630d2-68a3-4c7d-d960-749597b88f9a"
      },
      "source": [
        "train(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting Dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91)\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-aabc5be2a17e5dd4.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-085fbe7a91131577.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-7d63db416c5a4f8e.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-3b5f36296ad45978.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-dec2ac9a28c26d40.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-61e4aab25e87dfe6.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset Prepared!\n",
            "Loading Model...\n",
            "Model Loaded!\n",
            "Starting Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='2318' max='2318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2318/2318 15:42, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.287300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.189300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>3.069300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>3.014500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Starting Evaluation...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30/30 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation Metrics:\n",
            "Eval Loss: 3.044163942337036\n",
            "Perplexity: 20.992472946173525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAmjPHqbipPg"
      },
      "source": [
        "# Generate Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ers3L3mio8NT"
      },
      "source": [
        "Load the fine-tuned model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsVH8l_fgwo-"
      },
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "\n",
        "tokenizer = transformers.GPT2Tokenizer.from_pretrained(config.output_dir)\n",
        "model = transformers.GPT2LMHeadModel.from_pretrained(config.output_dir)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIqRue3TmWWB"
      },
      "source": [
        "Snippet for generation taken from: https://towardsdatascience.com/fine-tuning-gpt2-on-colab-gpu-for-free-340468c92ed\n",
        "\n",
        "`choose_from_top` will choose a token from its probability _p_ out of the top `n` most probable words.\n",
        "\n",
        "`generate` will generate a sentence given an input text, model and tokenizer. The `length` will define how long the generated sentence would be while `n` defines how many probabilities to consider (top n tokens)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFmrnGvzrxcK"
      },
      "source": [
        "def choose_from_top(probs, n=5):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob)\n",
        "    choice = np.random.choice(n, 1, p=top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)\n",
        "\n",
        "def generate(input_str, model, tokenizer, length=250, n=5):\n",
        "    cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(length):\n",
        "            outputs = model(cur_ids[:, -1024:], labels=cur_ids[:, -1024:])\n",
        "            loss, logits = outputs[:2]\n",
        "            softmax_logits = torch.softmax(logits[0, -1], dim=0)\n",
        "            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n)\n",
        "            cur_ids = torch.cat([cur_ids, torch.ones((1, 1)).long().to(device) * next_token_id], dim=1)\n",
        "        output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "        output_text = tokenizer.decode(output_list)\n",
        "        return output_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0ezWvdifv4Y",
        "outputId": "e12d808c-1bed-4d8b-c460-83c482b11021"
      },
      "source": [
        "generated_text = generate(\"= Interstellar =\", model, tokenizer)\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "= Interstellar = \n",
            " The film's title refers to the first time in history that humans have made a space trip. It also describes a time where humans traveled through space, as well as other worlds. The story of the film follows a group of astronauts who travel to Mars, where they meet the extraterrestrial race known as the \" Voyage Express, a race of extraterrestrials that has been searching for humans since the very beginning. \" \n",
            " = = Production = = \n",
            " The first draft script was written by John Travolta and directed by David Koeppner. The script was written by John Travolta and directed by David Koeppner and was directed by David Koeppner and Michael Cera. The script was written by John Travolta and directed by Michael Cera. The film was shot in a 3 @,@ 000 x 3 @,@ 000 @,@ inch format and had a budget of $ 1 @.@ 8 million. It also had a crew consisting of John Travolta, John Travolta, Michael Cera, and James Franco. \n",
            " The film was shot during the summer in Los Angeles. The location had been previously used for the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sar-skJVjSS1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}